---
title: "Trimming_Dry_Tortugas_temp_2023"
format:
  html:
    theme: default
---


# Cleaning the Dry Tortugas Temperature data: Trimming Each CSV

### Author: Maddy Enda
### Contact: madison.enda@myfwc.com
### Written: 01/30/2026


## Overview:

After compiling all of the years of temperature data in Dry Tortugas National Park (2006-2025), I found there were multiple entries for some locations at the same exact time. To get to the bottom of this mystery, we are going to trim any data that may have been recorded after the sensors were removed from the water.

The dates and times that each csv should be trimmed to are in a logger data file on the shared temperature drive for the FWRI Coral Research Program.

## Import libraries

```{r}
# Using librarian::shelf to import many libraries at once
librarian::shelf(tidyverse, dplyr, here, purrr, hms, lubridate, stringr)
```


____________________________________________________________________________________________________________________

# Creating functionality for all 2022-2023 files
## Cleaning function:

```{r}
# Function to clean the files

# Lookup table for site names based on source_file
site_lookup <- tibble(
  source_file = c(
    "BCR_Off1_22-23.csv",
    "BirdKey_Off1_22-23.csv",
    "Davis_Off1_22-23.csv",
    "Loggerhead_Off1_22-23.csv",
    "Mayers_Off2_22-23.csv",
    "Maze_Off1_22-23.csv",
    "Palmata_Off2_22-23.csv",
    "Prolifera_In2_22-23.csv",
    "Temptation_Off2_22-23.csv",
    "Texas_In2_22-23.csv",
    "WhiteShoal_Off3_22-23.csv"
  ),
  site_name = c(
    "Black Coral Rock",
    "Bird Key Reef",
    "Davis Rock",
    "Loggerhead Patch",
    "Mayer's Peak",
    "The Maze",
    "Palmata Patch",
    "Prolifera Patch",
    "Temptation Rock",
    "Texas Rock",
    "White Shoal"
  )
)

# --- Function to clean a single CSV ---
clean_temp_csv <- function(file_path, n_cols = 6, sep = ",") {
  
  df <- read_csv(file_path, col_names = FALSE, show_col_types = FALSE) %>%
    setNames("raw_data") %>%
    separate(
      col = raw_data,
      into = paste0("col", 1:n_cols),
      sep = sep,
      fill = "right",
      extra = "drop",
      convert = TRUE
    ) %>%
    slice(-(1:2)) %>%                     # remove first two rows
    mutate(source_file = basename(file_path)) %>%
    select(col1, col2, col3, source_file) %>%
    rename(
      observation_id = col1,
      datetime_raw   = col2,
      temp_F         = col3
    ) %>%
    filter(!is.na(temp_F), temp_F != "") %>%
    filter(!grepl("[^0-9]", observation_id)) %>%
    mutate(
      # Parse date/time from the raw datetime column
      datetime = mdy_hms(datetime_raw),
      date     = as.Date(datetime),
      time     = as_hms(format(datetime, "%H:%M:%S")),
      temp_F   = as.numeric(temp_F),
      temp_C   = (temp_F - 32) / 1.8,
      observation_id = as.integer(observation_id)
    ) %>%
    left_join(site_lookup, by = "source_file") %>%
    select(-datetime_raw, -source_file) %>%
    select(observation_id, date, time, datetime, everything())
  
  return(df)
}
```

# Trimming function:

```{r}
# Let's unpack one file, trim it, and then automate our process
# I will first create a data frame with the data from the Temp Logger Metadata File

# Create vectors of the data 
site_id <- c(82,83,28,46,45,27,42,43,44,29,41)

site_name <- c("Bird Key Reef","Black Coral Rock","Davis Rock","Loggerhead Patch","Mayer's Peak","The Maze","Palmata Patch","Prolifera Patch", "Temptation Rock","Texas Rock","White Shoal")

location <- c("Off 1","Off 1","Off 1","Off 1","Off 2","Off 1","Off 2","In 2","Off 2","In 2","Off 3")

logger_sn <- c(20868472,20364170,20868470,21151646,20668467,21151660,20606816,20423767,20868479,21151651,20423765)


deployment_date <- as.Date(c(
  "2022-08-14","2022-08-13","2022-08-12","2022-08-15","2022-08-11",
  "2022-08-11","2022-08-15","2022-08-15","2022-08-11","2022-08-12","2022-08-12"
))

deployment_time <- hms::as_hms(c(
  "11:00:00","08:30:00","11:00:00","15:00:00","10:00:00",
  "14:00:00","11:35:00","09:00:00","17:30:00","08:25:00","13:57:00"
))

retrieval_date <- as.Date(c(
  "2023-08-09","2023-08-11","2023-08-12","2023-08-10","2023-08-10",
  "2023-08-10","2023-08-09","2023-08-09","2023-08-11","2023-08-08","2023-08-10"
))

retrieval_time <- hms::as_hms(c(
  "11:06:00","09:00:00","08:50:00","09:13:00","13:43:00",
  "16:44:00","11:06:00","09:07:00","14:15:00","14:03:00","10:21:00"
))

# Create the data frame from the vectors
date_ranges_22_23 <- data.frame(
  site_id        = site_id,
  site_name   = site_name,
  location     = location,
  logger_sn = logger_sn,
  deployment_date = deployment_date,
  deployment_time = deployment_time,
  retrieval_date = retrieval_date,
  retrieval_time = retrieval_time,
  stringsAsFactors = FALSE
)

# Create a trimming function based on this table
trim_temp_csv <- function(cleaned_df, date_ranges_df) {
  
  date_ranges_df <- date_ranges_df %>%
    mutate(
      deploy_dt  = as.POSIXct(deployment_date) + deployment_time,
      retrieve_dt = as.POSIXct(retrieval_date) + retrieval_time
    ) %>%
    select(site_name, deploy_dt, retrieve_dt)
  
  cleaned_df %>%
    left_join(date_ranges_df, by = "site_name") %>%
    filter(datetime >= deploy_dt, datetime <= retrieve_dt) %>%
    select(-c(deploy_dt, retrieve_dt, datetime))
}
```

# Now let's try running this for one file!

```{r}
# Create a file path to our csv file
file_path <- "C:/Users/Madison.enda/Desktop/Temp_Data/2023/BCR_Off1_22-23.csv"

# Step 1: Clean CSV
BCR_clean <- clean_temp_csv(file_path)

# Step 2: Trim to deployment/retrieval window
BCR_trimmed <- trim_temp_csv(BCR_clean, date_ranges_22_23)
```


# OK, time to try them all!

```{r}
# Grab all the 2022-2023 files
files <- list.files(
  "C:/Users/Madison.enda/Desktop/Temp_Data/2023",
  pattern = "\\.csv$", 
  full.names = TRUE
)

dt_temp_2023 <- map_dfr(files, function(file) {
  clean_temp_csv(file) %>% 
    trim_temp_csv(date_ranges_22_23)
})
```


## Check daily counts

```{r}
# Check daily counts
daily_counts <- dt_temp_2023 %>%
  mutate(year = year(date)) %>%
  group_by(year, site_name, date) %>%
  summarise(
    n_obs = n(),
    .groups = "drop"
  )

# Let's find our problem days, that are not equal to 24
problem_days <- daily_counts %>%
  filter(n_obs != 24) %>%
  arrange(year, site_name, date)
```


## Write the final file

```{r}
# Write the cleaned and trimmed 2022-2023 file to our master file folder
write.csv(dt_temp_2023, "C:/Users/Madison.Enda/Desktop/Temp_Data/temp_dry_tortugas_2026_update/DRTO_2023_Master.csv")
```
