---
title: "Florida_Keys_Temp_2025"
author: "Maddy Enda"
format: html
editor: visual
---

# Temperature Data 2026 Update: Florida Keys National Marine Sanctuary

### Author- Madison Enda, February 5th, 2026

### Contact- madison.enda\@myfwc.com

## Overview:

-   In this script, we will be combining all 2023 temperature files from the Florida Key National Marine Sanctuary: pulling them off of the FWRI Coral Research Program's shared drive and creating an index to be used as our primary key for the 'Temperature' table in the upcoming database. For ease of access (and to avoid syncing with SharePoint which proved to take some time),the folder was downloaded manually to my device, and thus my file path is unique to my device.

-   This workflow will include general cleaning (standardizing naming conventions, removing superfluous data, etc.) and the final result will be uploaded to the shared drive once again, until permissions are received to migrate to a new platform.

## Import Libraries

```{r}
# Install librarian for multiple packages and quick readability
# install.packages("librarian")

# Using librarian::shelf to import many libraries at once
librarian::shelf(tidyverse, dplyr, here, purrr, hms, lubridate, stringr)
```

# Creating functionality for all 2022-2023 files

## Cleaning function:

- First, let's get all the file names that we will be converting into site_names

```{r}
# List the files in the 2025 folder
list.files(path = "c:/Users/Madison.Enda/Desktop/Temp_Data/2025")
```
_ Next, we create the cleaning function with the new files

```{r}
# Function to clean the files

# Lookup table for site names based on source_file
site_lookup <- tibble(
  source_file = c(
  "Admiral_Off4_24-25.csv",
  "AlligatorDp_Off3_24-25.csv",
  "AlligatorSh_Off3_24-25.csv",
  "Burr_Off1_24-25.csv",
  "CarysfortDp_In1_24-25.csv",
  "CarysfortSh_Off3_24-25.csv",
  "Cliff_Off3_24-25.csv",
  "ConchDp_Off3_24-25.csv",
  "ConchSh_Off3_24-25.csv",
  "Contents_Off3_24-25.csv",
  "Dove_Off1_24-25.csv",
  "Dustan_Off2_24-25.csv",
  "ESamboDeep_Off2_24-25.csv",
  "Grecian_In3_24-25.csv",
  "Jaap_Off1_24-25.csv",
  "Long_Off1_24-25.csv",
  "LooeDeep_Off1_24-25.csv",
  "LooeShallow_Off3_24-25.csv",
  "MolassesDp_Off2_24-25.csv",
  "MolassesSh_Off2_24-25.csv",
  "Moser_Off1_24-25.csv",
  "Porter_Off2_24-25.csv",
  "Radabob_Off2_24-25.csv",
  "Rattlesnake_Off1_24-25.csv",
  "Rawa_Off1_24-25.csv",
  "RedDun_Off1_24-25.csv",
  "RockDeep_Off3_24-25.csv",
  "RockShallow_Off4_24-25.csv",
  "SandDeep_In2_24-25.csv",
  "SandShallow_Off2_24-25.csv",
  "Smith_Off3_24-25.csv",
  "SombreroDeep_In1_24-25.csv",
  "SombreroShallow_Off2_24-25.csv",
  "Thor_Off2_24-25.csv",
  "TNDeep_In2_24-25.csv",
  "TNShallow_In2_24-25.csv",
  "Turtle_Off1_24-25.csv",
  "TwoPatches_Off2_24-25.csv",
  "WHead_In2_24-25.csv",
  "Wonderland_Off1_24-25.csv",
  "WSamboDeep_Off1_24-25.csv",
  "WSamboShallow_Off1_24-25.csv",
  "WTurtle_Off2_24-25.csv",
  "WWasher_Off3_24-25.csv"  
    ),
  
  site_name = c(
     "Admiral",
     "Alligator Deep",
     "Alligator Shallow",
     "Burr Fish",
     "Carysfort Deep",
     "Carysfort Shallow",
     "Cliff Green",
     "Conch Deep",
     "Conch Shallow",
     "Content Keys",
     "Dove Key",
     "Dustan Rocks",
     "Eastern Sambo Deep",
     "Grecian Rocks",
     "Jaap Reef",
     "Long Key",
     "Looe Key Deep",
     "Looe Key Shallow",
     "Molasses Deep",
     "Molasses Shallow",
     "Moser Channel",
     "Porter Patch",
     "El Radabob",
     "Rattlesnake",
     "Rawa Reef",
     "Red Dun Reef",
     "Rock Key Deep",
     "Rock Key Shallow",
     "Sand Key Deep",
     "Sand Key Shallow",
     "Smith Shoal",
     "Sombrero Deep",
     "Sombrero Shallow",
     "Thor",
     "Tennessee Deep",
     "Tennessee Shallow",
     "Turtle",
     "Two Patches",
     "Western Head",
     "Wonderland",
     "Western Sambo Deep",
     "Western Sambo Shallow",
     "West Turtle Shoal",
     "West Washerwoman"
  )
)

# --- Function to clean a single CSV ---
clean_temp_csv <- function(file_path, n_cols = 6, sep = ",") {
  
  df <- read_csv(file_path, col_names = FALSE, show_col_types = FALSE) %>%
    setNames("raw_data") %>%
    separate(
      col = raw_data,
      into = paste0("col", 1:n_cols),
      sep = sep,
      fill = "right",
      extra = "drop",
      convert = TRUE
    ) %>%
    slice(-(1:2)) %>%                     # remove first two rows
    mutate(source_file = basename(file_path)) %>%
    select(col1, col2, col3, source_file) %>%
    rename(
      observation_id = col1,
      datetime_raw   = col2,
      temp_F         = col3
    ) %>%
    filter(!is.na(temp_F), temp_F != "") %>%
    filter(!grepl("[^0-9]", observation_id)) %>%
    mutate(
      # Parse date/time from the raw datetime column
      datetime = mdy_hms(datetime_raw),
      date     = as.Date(datetime),
      time     = as_hms(format(datetime, "%H:%M:%S")),
      temp_F   = as.numeric(temp_F),
      temp_C   = (temp_F - 32) / 1.8,
      observation_id = as.integer(observation_id)
    ) %>%
    left_join(site_lookup, by = "source_file") %>%
    select(-datetime_raw, -source_file) %>%
    select(observation_id, date, time, datetime, everything())
  
  return(df)
}
```

## Trimming function:

```{r}
# Load in the date trim file created from deployment dates on the OneDrive
florida_keys_temp_2025_dates <- read_csv("C:/Users/Madison.Enda/Desktop/Temp_Data/florida_keys_temp_2025_dates.csv")

# Separate the dates and times

cleaning_dates <- florida_keys_temp_2025_dates %>%
  mutate(
    deployment_dt = mdy_hm(deployment_date, quiet = TRUE),
    retrieval_dt  = mdy_hm(retrieval_date, quiet = TRUE)
  ) %>%
  mutate(
    deployment_date = as.Date(deployment_dt),
    deployment_time = hms::as_hms(deployment_dt),
    retrieval_date  = as.Date(retrieval_dt),
    retrieval_time  = hms::as_hms(retrieval_dt)
  ) %>%
  select(-deployment_dt, -retrieval_dt)

# Create a trimming function based on this table
trim_temp_csv <- function(cleaned_df, date_ranges_df) {
  
  date_ranges_df <- date_ranges_df %>%
    mutate(
      deploy_dt  = as.POSIXct(deployment_date) + deployment_time,
      retrieve_dt = as.POSIXct(retrieval_date) + retrieval_time
    ) %>%
    select(site_name, deploy_dt, retrieve_dt)
  
  cleaned_df %>%
    left_join(date_ranges_df, by = "site_name") %>%
    filter(datetime >= deploy_dt, datetime <= retrieve_dt) %>%
    select(-c(deploy_dt, retrieve_dt, datetime))
}
```

## Time to load and trim the 2023 files!

```{r}
# OK, time to try them all!
files <- list.files(
  "C:/Users/Madison.enda/Desktop/Temp_Data/2025",
  pattern = "\\.csv$", 
  full.names = TRUE
)

fk_temp_2025 <- map_dfr(files, function(file) {
  clean_temp_csv(file) %>% 
    trim_temp_csv(cleaning_dates)
})
```

## Check for anomalies?

```{r}
# Look for NA values
colSums(is.na(fk_temp_2025))
```

## Write file as master list for 2023

```{r}
# Write the final data frame to a csv in my current working directory
write.csv(fk_temp_2025, "florida_keys_temp_2025.csv", row.names = FALSE)
```
