---
title: "Florida_Keys_Temp_2024"
author: "Maddy Enda"
format: html
editor: visual
---

# Temperature Data 2026 Update: Florida Keys National Marine Sanctuary

### Author- Madison Enda, February 6th, 2026

### Contact- madison.enda\@myfwc.com

## Overview:

-   In this script, we will be combining all 2024 temperature files from the Florida Key National Marine Sanctuary: pulling them off of the FWRI Coral Research Program's shared drive and creating an index to be used as our primary key for the 'Temperature' table in the upcoming database. For ease of access (and to avoid syncing with SharePoint which proved to take some time),the folder was downloaded manually to my device, and thus my file path is unique to my device.

-   This workflow will include general cleaning (standardizing naming conventions, removing superfluous data, etc.) and the final result will be uploaded to the shared drive once again, until permissions are received to migrate to a new platform.

## Import Libraries

```{r}
# Install librarian for multiple packages and quick readability
# install.packages("librarian")

# Using librarian::shelf to import many libraries at once
librarian::shelf(tidyverse, dplyr, here, purrr, hms, lubridate, stringr)
```

# Creating functionality for all 2023-2024 files

## Cleaning function: Winter Temp Loggers

```{r}
# Function to clean the files

# Lookup table for site names based on source_file
site_lookup <- tibble(
  source_file = c("Admiral_Off4_23-24.csv",
                  "AlligatorDeep_Off3_23-24.csv",
                  "AlligatorShallow_Off3_23-24.csv",
                  "Burr_Off1_23-24.csv",
                  "CarysfortDeep_In1_23-24.csv",
                  "CarysfortShallow_Off3_23-24.csv",
                  "CliffGreen_Off3_22-23.csv",
                  "ConchDeep_Off3_23-24.csv",
                  "ConchShallow_Off3_23-24.csv",
                  "Content_Off3_23-24.csv",
                  "Dove_Off1_23-24.csv",
                  "Dustan_Off2_23-24.csv",
                  "ElRadabob_Off2_23-24.csv",
                  "ESamboDeep_Off2_23-24.csv",
                  "Grecian_In3_23-24.csv",
                  "Jaap_Off1_23-24.csv",
                  "Long_Off1_23-24.csv",
                  "LooeDeep_Off1_23-24.csv",
                  "LooeShall_Off1_23-24.csv",
                  "MolassesDeep_Off2_23-24.csv",
                  "MolassesShallow_Off2_23-24.csv",
                  "Moser_Off2_23-24.csv",
                  "Porter_Off1_23-24.csv",
                  "Rattlesnake_Off1_23-24.csv",
                  "Rawa_Off1_23-24.csv",
                  "RedDun_Off1_23-24.csv",
                  "RockDeep_Off3_23-24.csv",
                  "RockShall_Off4_23-24.csv",
                  "SandDeep_In2_23-24.csv",
                  "SandShall_Off2_23-24.csv",
                  "Smith_In2_23-24.csv",
                  "SombreroDeep_Off1_23-24.csv",
                  "SombShall_Off3_23-24.csv",
                  "TennDeep_In2_23-24.csv",
                  "TennShall_In2_23-24.csv",
                  "Thor_23-24.csv",
                  "Turtle_Off1_23-24.csv",
                  "TwoPatches_Off1_23-24.csv",
                  "WesternHead_In2_23-24.csv",
                  "WestTurt_Off2_23-24.csv",
                  "WestWasher_Off1_23-24.csv",
                  "Wonderland_Off1_23-24.csv",
                  "WSamboDeep_Off1_23-24.csv",
                  "WSamboShall_Off1_23-24.csv"),
  site_name = c(
     "Admiral",
     "Alligator Deep",
     "Alligator Shallow",
     "Burr Fish",
     "Carysfort Deep",
     "Carysfort Shallow",
     "Cliff Green",
     "Conch Deep",
     "Conch Shallow",
     "Content Keys",
     "Dove Key",
     "Dustan Rocks",
     "El Radabob",
     "Eastern Sambo Deep",
     "Grecian Rocks",
     "Jaap Reef",
     "Long Key",
     "Looe Key Deep",
     "Looe Key Shallow",
     "Molasses Deep",
     "Molasses Shallow",
     "Moser Channel",
     "Porter Patch",
     "Rattlesnake",
     "Rawa Reef",
     "Red Dun Reef",
     "Rock Key Deep",
     "Rock Key Shallow",
     "Sand Key Deep",
     "Sand Key Shallow",
     "Smith Shoal",
     "Sombrero Deep",
     "Sombrero Shallow",
     "Tennessee Deep",
     "Tennessee Shallow",
     "Thor",
     "Turtle",
     "Two Patches",
     "Western Head",
     "West Turtle Shoal",
     "West Washerwoman",
     "Wonderland",
     "Western Sambo Deep",
     "Western Sambo Shallow"
  )
)

# --- Function to clean a single CSV ---
clean_temp_csv <- function(file_path, n_cols = 6, sep = ",") {
  
  df <- read_csv(file_path, col_names = FALSE, show_col_types = FALSE) %>%
    setNames("raw_data") %>%
    separate(
      col = raw_data,
      into = paste0("col", 1:n_cols),
      sep = sep,
      fill = "right",
      extra = "drop",
      convert = TRUE
    ) %>%
    slice(-(1:2)) %>%                     # remove first two rows
    mutate(source_file = basename(file_path)) %>%
    select(col1, col2, col3, source_file) %>%
    rename(
      observation_id = col1,
      datetime_raw   = col2,
      temp_F         = col3
    ) %>%
    filter(!is.na(temp_F), temp_F != "") %>%
    filter(!grepl("[^0-9]", observation_id)) %>%
    mutate(
      # Parse date/time from the raw datetime column
      datetime = mdy_hms(datetime_raw),
      date     = as.Date(datetime),
      time     = as_hms(format(datetime, "%H:%M:%S")),
      temp_F   = as.numeric(temp_F),
      temp_C   = (temp_F - 32) / 1.8,
      observation_id = as.integer(observation_id)
    ) %>%
    left_join(site_lookup, by = "source_file") %>%
    select(-datetime_raw, -source_file) %>%
    select(observation_id, date, time, datetime, everything())
  
  return(df)
}
```

## Trimming function: Winter Temp loggers

```{r}
# Load in the date trim file created from deployment dates on the OneDrive
florida_keys_temp_2024_dates <- read_csv("C:/Users/Madison.Enda/Desktop/Temp_Data/florida_keys_temp_2024_dates.csv")

# Separate the dates and times

cleaning_dates <- florida_keys_temp_2024_dates %>%
  mutate(
    deployment_dt = mdy_hm(deployment_date, quiet = TRUE),
    retrieval_dt  = mdy_hm(retrieval_date, quiet = TRUE)
  ) %>%
  mutate(
    deployment_date = as.Date(deployment_dt),
    deployment_time = hms::as_hms(deployment_dt),
    retrieval_date  = as.Date(retrieval_dt),
    retrieval_time  = hms::as_hms(retrieval_dt)
  ) %>%
  select(-deployment_dt, -retrieval_dt)

# Create a trimming function based on this table
trim_temp_csv <- function(cleaned_df, date_ranges_df) {
  
  date_ranges_df <- date_ranges_df %>%
    mutate(
      deploy_dt  = as.POSIXct(deployment_date) + deployment_time,
      retrieve_dt = as.POSIXct(retrieval_date) + retrieval_time
    ) %>%
    select(site_name, deploy_dt, retrieve_dt)
  
  cleaned_df %>%
    left_join(date_ranges_df, by = "site_name") %>%
    filter(datetime >= deploy_dt, datetime <= retrieve_dt) %>%
    select(-c(deploy_dt, retrieve_dt, datetime))
}
```


## Load in and trim the winter 2024 files

```{r}
# Load through each of the 2024 winter files (deployed in summer of 2023 and picked up in winter of 2024)
files <- list.files(
  "C:/Users/Madison.enda/Desktop/Temp_Data/2024",
  pattern = "\\.csv$", 
  full.names = TRUE
)

fk_temp_2024_W <- map_dfr(files, function(file) {
  clean_temp_csv(file) %>% 
    trim_temp_csv(cleaning_dates)
})
```



## Cleaning Function: Summer Temp Loggers

```{r}
# Function to clean the files

# Lookup table for site names based on source_file
site_lookup <- tibble(
     source_file = c("Admiral_Off4_Winter-Summer24.csv",
                  "AlligatorDeep_Off3_Winter-Summer24.csv",
                  "AlligatorShallow_Off3_Winter-Summer24.csv",
                  "Burr_Off1_Winter-Summer24.csv",
                  "CarysfortDeep_In1_Winter-Summer24.csv",
                  "CarysfortShallow_Off3_Winter-Summer24.csv",
                  "Cliff_Off3_Winter-Summer24.csv",
                  "ConchDeep_Off3_Winter-Summer24.csv",
                  "ConchShallow_Off3_Winter-Summer24.csv",
                  "Contents_Off3_Winter-Summer24.csv",
                  "Dove_Off1_Winter-Summer24.csv",
                  "Dustan_Off2_Winter-Summer24.csv",
                  "ESamboDp_Off2_Winter-Summer24.csv",
                  "ESamboSh_Off2_Winter-Summer24.csv",
                  "Grecian_In3_Winter-Summer24.csv",
                  "Jaap_Off1_Winter-Summer24.csv",
                  "Long_Off1_Winter-Summer24.csv",
                  "LooeDp_Off1_Winter-Summer24.csv",
                  "LooeSh_Off3_Winter-Summer24.csv",
                  "MolassesShallow_Off2_Winter-Summer24.csv",
                  "Moser_Off1_Winter-Summer24.csv",
                  "Porter_Off2_Winter-Summer24.csv",
                  "Radabob_Off2_Winter-Summer24.csv",
                  "Rattle_Off1_Winter-Summer24.csv",
                  "Rawa_Off1_Winter-Summer24.csv",
                  "RedDun_Off1_Winter-Summer24.csv",
                  "RockDp_Off3_Winter-Summer24.csv",
                  "RockSh_Off4_Winter-Summer24.csv",
                  "SandDp_In2_Winter-Summer24.csv",
                  "SandSh_Off2_Winter-Summer24.csv",
                  "Smith_Off3_Winter-Summer24.csv",
                  "SombDp_In1_Winter-Summer24.csv",
                  "SombreroDeep_Off1_23-24.csv",
                  "Thor_Off2_Winter-Summer24.csv",
                  "TNDp_In2_Winter-Summer24.csv",
                  "TNSh_In2_Winter-Summer24.csv",
                  "Turtle_Off1_Winter-Summer24.csv",
                  "TwoPatches_Off2_Winter-Summer24.csv",
                  "WHead_In2_Winter-Summer24.csv",
                  "Wonder_Off1_Winter-Summer24.csv",
                  "WSamboDp_Off1_Winter-Summer24.csv",
                  "WSamboSh_Off1_Winter-Summer24.csv",
                  "WTurtleShoal_Off2_Winter-Summer24.csv",
                  "WWasher_Off3_Winter-Summer24.csv"),
  site_name = c(
     "Admiral",
     "Alligator Deep",
     "Alligator Shallow",
     "Burr Fish",
     "Carysfort Deep",
     "Carysfort Shallow",
     "Cliff Green",
     "Conch Deep",
     "Conch Shallow",
     "Content Keys",
     "Dove Key",
     "Dustan Rocks",
     "Eastern Sambo Deep",
     "Eastern Sambo Shallow",
     "Grecian Rocks",
     "Jaap Reef",
     "Long Key",
     "Looe Key Deep",
     "Looe Key Shallow",
     "Molasses Shallow",
     "Moser Channel",
     "Porter Patch",
     "El Radobob",
     "Rattlesnake",
     "Rawa Reef",
     "Red Dun Reef",
     "Rock Key Deep",
     "Rock Key Shallow",
     "Sand Key Deep",
     "Sand Key Shallow",
     "Smith Shoal",
     "Sombrero Deep",
     "Sombrero Shallow",
     "Thor",
     "Tennessee Deep",
     "Tennessee Shallow",
     "Turtle",
     "Two Patches",
     "Western Head",
     "Wonderland",
     "Western Sambo Deep",
     "Western Sambo Shallow",
     "West Turtle Shoal",
     "West Washerwoman"
  )
)

# --- Function to clean a single CSV ---
clean_temp_csv <- function(file_path, n_cols = 6, sep = ",") {
  
  df <- read_csv(file_path, col_names = FALSE, show_col_types = FALSE) %>%
    setNames("raw_data") %>%
    separate(
      col = raw_data,
      into = paste0("col", 1:n_cols),
      sep = sep,
      fill = "right",
      extra = "drop",
      convert = TRUE
    ) %>%
    slice(-(1:2)) %>%                     # remove first two rows
    mutate(source_file = basename(file_path)) %>%
    select(col1, col2, col3, source_file) %>%
    rename(
      observation_id = col1,
      datetime_raw   = col2,
      temp_F         = col3
    ) %>%
    filter(!is.na(temp_F), temp_F != "") %>%
    filter(!grepl("[^0-9]", observation_id)) %>%
    mutate(
      # Parse date/time from the raw datetime column
      datetime = mdy_hms(datetime_raw),
      date     = as.Date(datetime),
      time     = as_hms(format(datetime, "%H:%M:%S")),
      temp_F   = as.numeric(temp_F),
      temp_C   = (temp_F - 32) / 1.8,
      observation_id = as.integer(observation_id)
    ) %>%
    left_join(site_lookup, by = "source_file") %>%
    select(-datetime_raw, -source_file) %>%
    select(observation_id, date, time, datetime, everything())
  
  return(df)
}
```


## Trimming function: Summer Temp Loggers

```{r}
# Load in the date trim file created from deployment dates on the OneDrive
florida_keys_temp_2024_dates <- read_csv("C:/Users/Madison.Enda/Desktop/Temp_Data/florida_keys_temp_2024_summer_dates.csv")

# Separate the dates and times
cleaning_dates <- florida_keys_temp_2024_dates %>%
  mutate(
    deployment_dt = mdy_hm(deployment_date, quiet = TRUE),
    retrieval_dt  = mdy_hm(retrieval_time, quiet = TRUE)
  ) %>%
  mutate(
    deployment_date = as.Date(deployment_dt),
    deployment_time = hms::as_hms(deployment_dt),
    retrieval_date  = as.Date(retrieval_dt),
    retrieval_time  = hms::as_hms(retrieval_dt)
  ) %>%
  select(-deployment_dt, -retrieval_dt)

# Create a trimming function based on this table
trim_temp_csv <- function(cleaned_df, date_ranges_df) {
  
  date_ranges_df <- date_ranges_df %>%
    mutate(
      deploy_dt  = as.POSIXct(deployment_date) + deployment_time,
      retrieve_dt = as.POSIXct(retrieval_date) + retrieval_time
    ) %>%
    select(site_name, deploy_dt, retrieve_dt)
  
  cleaned_df %>%
    left_join(date_ranges_df, by = "site_name") %>%
    filter(datetime >= deploy_dt, datetime <= retrieve_dt) %>%
    select(-c(deploy_dt, retrieve_dt, datetime))
}
```

## Load in and trim the summer 2024 files

```{r}
# Load through each of the 2024 winter files (deployed in summer of 2023 and picked up in winter of 2024)
files <- list.files(
  "C:/Users/Madison.enda/Desktop/Temp_Data/2024_summer",
  pattern = "\\.csv$", 
  full.names = TRUE
)

fk_temp_2024_S <- map_dfr(files, function(file) {
  clean_temp_csv(file) %>% 
    trim_temp_csv(cleaning_dates)
})
```


## Bind togther the winter and summer data sets

```{r}
# Bind together the sets with bind_rows()
fk_temp_2024 <- bind_rows(fk_temp_2024_W, fk_temp_2024_S)
```

## Write the master list for 2024

```{r}
# Write the final data frame to a csv in my current working directory
write.csv(fk_temp_2024, "florida_keys_temp_2024.csv", row.names = FALSE)
```



